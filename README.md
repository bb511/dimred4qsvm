[![Email: patrick](https://img.shields.io/badge/email-podagiu%40student.ethz.ch-blue?style=flat-square&logo=minutemailer)](mailto:podagiu@student.ethz.ch)
[![Email: vasilis](https://img.shields.io/badge/email-vasileios.belis%40cern.ch-blue?style=flat-square&logo=minutemailer)](mailto:vasileios.belis@cern.ch)
[![Python: version](https://img.shields.io/badge/python-3.8-blue?style=flat-square&logo=python)](https://www.python.org/downloads/)
[![License: version](https://img.shields.io/badge/license-MIT-purple?style=flat-square)](https://github.com/QML-HEP/ae_qml/blob/main/LICENSE)
[![Code style: black](https://img.shields.io/badge/code%20style-black-black?style=flat-square&logo=black)](https://github.com/psf/black)

# Learning Reduced Representations for Quantum Classifiers

Data sets that are specified by a large number of features are currently outside the area of applicability for quantum machine learning algorithms. An immediate solution to this impasse is the application of dimensionality reduction methods before passing the data to the quantum algorithm.
We investigate six conventional feature extraction algorithms and five autoencoder-based dimensionality reduction models to a particle physics data set with 67 features. 
The reduced representations generated by these models are then used to train a quantum support vector machine for solving a binary classification problem: whether a Higgs boson is produced in proton collisions at the LHC.
We show that the autoencoder methods learn a better lower-dimensional representation of the data, with the method we design, the Sinkclass autoencoder, performing 40% better than the baseline.
The methods developed here open up the applicability of quantum machine learning to a larger array of data sets. 
Moreover, we provide a recipe for effective dimensionality reduction in this context.

This repository represents the source code of the following paper [Learning Reduced Representations for Quantum Classifiers](TBA)

If you plan to use or take part of the code, please cite the usage:
```
TBA
```

## Installing Dependencies

We strongly recommend using `conda` to install the dependencies for this repo.
If you have 'conda', go into the folder with the code you want to run, then create
an environment from the .yml file in that folder. Activate the environment.
Now you can run the code! Go to the *Running the code section.* for further instructions.

If you do not want to use `conda`, here is a list of the packages you
would need to install:

**Pre-processing**
* numpy
* pandas
* pytables
* matplotlib
* scikit-learn

**Auto-encoders**
* numpy
* matplotlib
* scikit-learn
* pytorch (follow instruction [here](https://pytorch.org))
* torchinfo
* pykeops
  * g++ compiler version >= 7
  * cudatoolkit version >= 10  
* geomloss

**QSVM**
* numpy
* matplotlib
* scikit-learn
* pytorch (follow instruction [here](https://pytorch.org))
* torchinfo
* pykeops
  * g++ compiler version >= 7
  * cudatoolkit version >= 10  
* geomloss
* qiskit

The pykeops package is required to run the Sinkhorn auto-encoder. However,
it is a tricky package to manage, so make sure that you have a gcc and a g++
compiler in your path that is compatible with the version of cuda you are
running. We recommend using conda for exactly this reason, since conda
sets certain environment variables such that everything is configured correctly
and pykeops can compile using cuda.

If you encounter any bugs, please contact us at the email addresses listed
on this repository.

## Running the Code

The data preprocessing scripts are ran from inside the preprocessing folder.
These scripts were customised for the specific data set that the authors are
using. For access to this data, please contact us.

The preprocessing scripts produce normalised numpy arrays saved to three
different files for training, validation, and testing.

The scripts to launch the autoencoder training on the data are in the bin
folder. *Look for the `run.snip` files to see the basic run cases for the*
*code and customise from there*.
